alias-free-torch>=0.0.6
descript-audio-codec>=1.0.0
diffusers==0.27.2
omegaconf==2.0.6
fairseq==0.12.2
einops>=0.8.1
einops-exts==0.0.4
flashy>=0.0.2
huggingface-hub==0.25.2
julius>=0.2.7
k-diffusion==0.1.1
kaldiio>=2.18.1
lameenc>=1.8.1
librosa>=0.11.0
lightning>=2.5.2
ninja>=1.11.1.4
nnAudio>=0.3.3
openunmix>=1.3.0
peft==0.10.0
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
transformers==4.37.2
vector-quantize-pytorch>=1.22.17
wheel>=0.45.1
x-transformers>=2.3.25
flash_attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl#sha256=ffe17686fa1a0f288de9eae7c32af209d32a27b037ef28614f042b377af5b15a
accelerate